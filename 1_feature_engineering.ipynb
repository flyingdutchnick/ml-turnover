{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc, warnings, random, datetime, math, awscli\n",
    "\n",
    "from pandas.util import hash_pandas_object\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, SimpleImputer\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and opening files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ceos = pd.read_pickle(\"./data/df_ceos.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ceos[\"place_time_hash\"] = hash_pandas_object(df_ceos[[\"date\",\"company_id\"]], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compression_opts = dict(method='gzip', archive_name='pre_processed_data.csv')\n",
    "# df_ceos.to_csv('./data/csv_files/pre_processed_data.csv.gz', index=False, compression=compression_opts)\n",
    "# df_ceos.to_pickle(\"./data/df_ceos.pkl\")\n",
    "# df_directors.to_pickle(\"./data/df_directors.pkl\")\n",
    "# df_share_prices.to_pickle(\"./data/df_share_prices.pkl\")\n",
    "# df_directors = pd.read_pickle(\"./data/df_directors.pkl\")\n",
    "# df_ceos.sort_values(['company_name', 'director_name'], ascending=[True,True], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO-DO\n",
    "Add this code in somewhere to run some cohort analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to download the CSV files to your local ./data/csv_files directory\n",
    "# Make sure you have configured your AWS Credential File\n",
    "!aws s3 sync s3://sagemaker-us-east-1-936165954724/ml-turnover/ ./data/csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Completed 167.6 MiB/171.8 MiB (2.7 MiB/s) with 1 file(s) remainingCompleted 167.8 MiB/171.8 MiB (2.7 MiB/s) with 1 file(s) remainingCompleted 168.1 MiB/171.8 MiB (2.7 MiB/s) with 1 file(s) remainingCompleted 168.3 MiB/171.8 MiB (2.7 MiB/s) with 1 file(s) remainingCompleted 168.6 MiB/171.8 MiB (2.7 MiB/s) with 1 file(s) remainingCompleted 168.8 MiB/171.8 MiB (2.7 MiB/s) with 1 file(s) remainingCompleted 169.1 MiB/171.8 MiB (2.7 MiB/s) with 1 file(s) remainingCompleted 169.3 MiB/171.8 MiB (2.7 MiB/s) with 1 file(s) remainingCompleted 169.6 MiB/171.8 MiB (2.7 MiB/s) with 1 file(s) remainingCompleted 169.8 MiB/171.8 MiB (2.7 MiB/s) with 1 file(s) remainingCompleted 170.1 MiB/171.8 MiB (2.7 MiB/s) with 1 file(s) remainingCompleted 170.3 MiB/171.8 MiB (2.7 MiB/s) with 1 file(s) remainingCompleted 170.6 MiB/171.8 MiB (2.7 MiB/s) with 1 file(s) remainingCompleted 170.8 MiB/171.8 MiB (2.7 MiB/s) with 1 file(s) remainingCompleted 171.1 MiB/171.8 MiB (2.7 MiB/s) with 1 file(s) remainingCompleted 171.3 MiB/171.8 MiB (2.7 MiB/s) with 1 file(s) remainingCompleted 171.6 MiB/171.8 MiB (2.7 MiB/s) with 1 file(s) remainingCompleted 171.8 MiB/171.8 MiB (2.7 MiB/s) with 1 file(s) remainingupload: data/csv_files/ceos.csv.gz to s3://sagemaker-us-east-1-936165954724/ml-turnover/ceos.csv.gz\n"
    }
   ],
   "source": [
    "# You can run this command to upload new CSV files to the remote directory\n",
    "!aws s3 sync ./data/csv_files s3://sagemaker-us-east-1-936165954724/ml-turnover/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load share price data for companies\n",
    "def load_gzip_csv_file(name, encoding='utf8'):\n",
    "    \"\"\"\n",
    "    Reads csv file (gzip) from data/csv_files directory\n",
    "    :param name: file name without extension\n",
    "    :return: dataframe\n",
    "    \"\"\"\n",
    "    csv_file = f'./data/csv_files/{name}.csv.gz'\n",
    "    df = pd.read_csv(csv_file, compression='gzip', encoding=encoding)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_datetime(df):\n",
    "    \"\"\"\n",
    "    Convert datedate into a pandas datetime format\n",
    "    \"\"\"\n",
    "    df[\"datadate\"] = pd.to_datetime(df.datadate, format=\"%Y%m%d\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load csv file\n",
    "def load_csv_file(name):\n",
    "    \"\"\"\n",
    "    Reads csv file (gzip) from data/csv_files directory\n",
    "    :param name: file name without extension\n",
    "    :return: dataframe\n",
    "    \"\"\"\n",
    "    csv_file = f'./data/csv_files/{name}.csv'\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(df, type, periods=[1, 3, 6, 12, 24, 36]):\n",
    "    \"\"\"\n",
    "    Return dataframe with monthly returns\n",
    "    :param df: dataframe with raw prices\n",
    "    :param type: firm, sector, market\n",
    "    :return: dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    for period in periods:\n",
    "        key_1 = str(type) + '_rtn_' + str(period) + 'm'\n",
    "        # key_2 = 'annualized_return_' + str(period) + 'm'\n",
    "        # use ffill method to deal with missing closing prices data\n",
    "        try: \n",
    "            df[key_1] = df.groupby(['cusip'])['price'].pct_change(fill_method='ffill', periods=period)\n",
    "        except: \n",
    "            df[key_1] = df['price'].pct_change(fill_method='ffill', periods=period)\n",
    "        # df[key_2] = ((df[key_1]+1)**(12/period)-1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sector_returns(df, periods=[1, 3, 6, 12, 24, 36]):\n",
    "    \"\"\"\n",
    "    Return dataframe with monthly returns\n",
    "    :param df: dataframe with raw prices\n",
    "    :return: dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    for period in periods:\n",
    "        lookup_key = f'firm_rtn_{period}m'\n",
    "        key = 'sector_rtn_' + str(period) + 'm'\n",
    "        # use ffill method to deal with missing closing prices data\n",
    "        df[key] = df.groupby(['sic', 'date'])[lookup_key].transform('mean')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_director_age():\n",
    "    \"\"\" Load csv file containing director ages\n",
    "    \n",
    "    Args: \n",
    "        input_data: csv file in S3 location\n",
    "\n",
    "    Returns: \n",
    "        dataframe with each row corresponding to a single role\n",
    "    \"\"\"\n",
    "    csv_file = './data/csv_files/boardex_director_age_nationality.csv'\n",
    "    df = pd.read_csv(csv_file, encoding='latin1')\n",
    "    # df.datadate = pd.to_datetime(df.datadate, format=\"%Y%m%d\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_df(dict, n):\n",
    "\n",
    "    if (n == 1):\n",
    "        return dict\n",
    "\n",
    "    else: \n",
    "        p = sum(dict.values())/2\n",
    "        _sum = 0\n",
    "        _dict = {}\n",
    "\n",
    "        while _sum < p:\n",
    "            removed_item = dict.popitem()\n",
    "            _dict[removed_item[0]] = removed_item[1]\n",
    "            _sum += removed_item[1]\n",
    "            \n",
    "        return (partition_df(_dict, n/2), partition_df(dict, n/2))\n",
    "\n",
    "_dict = df_ceos.director_id.to_dict()\n",
    "keyz = partition_df(_dict, 4)\n",
    "flat_keyz = [item for sublist in keyz for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ceos['nationality'] = df_ceos['nationality'].fillna('NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['first_bid'] = df.assign(date = df['date'].\\\n",
    "# where(df['event'] == 'bid')).groupby('user_id')['date'].transform('min')\n",
    "# df_ceos['company_tenure_at_ceo_role_start'] = df_ceos.assign(company_tenure_at_ceo_role_start = \\\n",
    "df_ceos.assign(company_tenure_at_ceo_role_start = df_ceos['company_roles_tenure'].\\\n",
    "    where(\\\n",
    "    (df_ceos['start_date'] == df_ceos['date'])\\\n",
    "    &(df_ceos['role_extension']==False)\\\n",
    "    ))\n",
    "    # .groupby(['director_id', 'company_id'])['date'].transform('min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ceos = df_ceos.dropna(subset=['firm_rtn_36m', 'previous_ceo_tenure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ceos[df_ceos.director_id == 181726][60:120][[\"date\",\"role_id\", \"role_duration\",\"role_tenure\", \"previous_ceo_tenure\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_nans(df):\n",
    "    nans_df = df.isna()\n",
    "\n",
    "    for col in df.columns:\n",
    "        cur_group = nans_df[col].sum()\n",
    "        if cur_group >= 0:\n",
    "            print(f'{col}')\n",
    "            print(f'NAN row count = {cur_group}')\n",
    "            print(f'% NAN count = {df[col].isnull().mean()*100:.2f}%\\n')\n",
    "        \n",
    "    del nans_df\n",
    "\n",
    "check_nans(df_ceos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load market index prices \n",
    "df_index_prices = load_gzip_csv_file('index_prices')\n",
    "df_index_prices = convert_datetime(df_index_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-processed data\n",
    "csv_file = './data/csv_files/pre_processed_data.csv.gz'\n",
    "df_pre_processed = pd.read_csv(csv_file, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load share price director link\n",
    "df_lookup_table = load_csv_file('boardex_capiq_link_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load remuneration data\n",
    "df_remuneration = load_gzip_csv_file('boardex_director_remuneration', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load director data\n",
    "df_directors = load_gzip_csv_file('boardex_director_profiles', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load age data\n",
    "df_ages = load_director_age()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2a: Clean up the share price dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load share prices data\n",
    "df_share_prices = load_gzip_csv_file('share_prices')\n",
    "df_share_prices = convert_datetime(df_share_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with more than 10% missing values\n",
    "df_share_prices = df_share_prices.loc[:, df_share_prices.isnull().mean() < .1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename some column names to identifiable names\n",
    "df_share_prices = df_share_prices.rename(columns={\"prccm\":\"raw_price\", \"ajexm\": \"adjustment_factor\", \"conm\": \"company\", \"datadate\": \"date\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop if the row is missing a SIC (industry) code\n",
    "df_share_prices = df_share_prices.dropna(subset=['sic', 'cusip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop all companies with an adjustment factor of 0 (can't divide by 0)\n",
    "print(f\"Number of rows before adjustment: {len(df_share_prices)}\")\n",
    "\n",
    "print(f\"Processing...\")\n",
    "\n",
    "moonshot_prices = df_share_prices[df_share_prices[\"adjustment_factor\"] == 0][\"gvkey\"].unique()\n",
    "print(f\"Number of companies with desparate reverse splits: {len(moonshot_prices)}\")\n",
    "\n",
    "df_share_prices = df_share_prices[~df_share_prices.gvkey.isin(moonshot_prices)]\n",
    "\n",
    "print(f\"Number of rows after adjustment: {len(df_share_prices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert raw price into adjusted price\n",
    "df_share_prices[\"price\"] = df_share_prices[\"raw_price\"]/df_share_prices[\"adjustment_factor\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate firm returns\n",
    "df_share_prices = calculate_returns(df_share_prices, 'firm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in the GVKEY to the directors dataframe \n",
    "# df_lookup_table = df_lookup_table.rename(columns={'companyid':'CompanyID'})\n",
    "# df_directors = df_directors.merge(df_lookup_table, on='CompanyID', how='left')\n",
    "\n",
    "# Find out which GVKEY in the performance dataset appears in the directors dataset\n",
    "gvkey_list = df_share_prices.gvkey.isin(df_ceos[\"gvkey\"])\n",
    "\n",
    "# Select only the gvkeys in the performance dataset that also occur in the director dataframe\n",
    "df_share_prices = df_share_prices[gvkey_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop penny stocks with a share price less than USD 0.01 or monthly returns in excess of 10x \n",
    "penny_stock_gvkeys = df_share_prices[(df_share_prices[\"raw_price\"] < 0.01) | (df_share_prices[\"firm_rtn_1m\"]>10)][\"gvkey\"].unique()\n",
    "df_share_prices = df_share_prices[~df_share_prices.gvkey.isin(penny_stock_gvkeys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop more unused columns containing no information\n",
    "df_share_prices = df_share_prices.drop(['iid', \"cusip\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop rows with nan in the price column\n",
    "print(f\"Number of rows before adjustment: {len(df_share_prices)}\")\n",
    "print(f\"Processing...\")\n",
    "df_share_prices.dropna(subset=[\"price\"], inplace=True)\n",
    "print(f\"Number of rows after dropping NaNs: {len(df_share_prices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sector returns\n",
    "df_share_prices = calculate_sector_returns(df_share_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save only the necessary columns\n",
    "df_share_prices.drop(['ajpm','company','state', 'raw_price', 'adjustment_factor', 'trfm', 'city', 'naics', 'price'], axis=1, errors='raise', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2b: Clean up the market index file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the Russell 3000 (broad market index)\n",
    "df_index = df_index_prices[df_index_prices[\"conm\"] == \"Russell 3000\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columsn with \"NA\" values\n",
    "df_index = df_index[[\"datadate\", \"prccm\"]].rename(columns={\"prccm\":\"price\", \"datadate\":\"date\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate index returns\n",
    "df_index = calculate_returns(df_index, 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in index returns\n",
    "df_share_prices = df_share_prices.merge(df_index, on=\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2c: Clean up the remuneration data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect the remuneration file\n",
    "df_remuneration[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to pandas datetime format, remove the AnnualReportDate column\n",
    "df_remuneration[\"date\"] = pd.to_datetime(df_remuneration[\"AnnualReportDate\"], format=\"%Y%m%d\")\n",
    "del df_remuneration[\"AnnualReportDate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map for renaming columns\n",
    "remuneration_cols_map = {\n",
    "    'BoardName':'company_name',\n",
    "    'DirectorName':'director_name',\n",
    "    'RoleName':'role_name',\n",
    "    'Currency':'currency',\n",
    "    'BoardID':'company_id',\n",
    "    'DirectorID':'director_id',\n",
    "    'Salary':'salary',\n",
    "    'Bonus':'bonus', \n",
    "    'Other':'other', \n",
    "    'PenEmpCon':'pension',\n",
    "    'TotalCompensation':'tot_comp', \n",
    "    'ValTotEqHeld':'equity_held',\n",
    "    'TotRemPeriod':'tot_remuneration',\n",
    "    'TotalDirectComp':'tot_direct_comp'\n",
    "}\n",
    "df_remuneration = df_remuneration.rename(columns = remuneration_cols_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only rows in df_remuneration dataframe where company_id occurs in df_ceos dataframe \n",
    "df_remuneration_filter = df_remuneration['company_id'].isin(df_ceos[\"company_id\"])\n",
    "df_remuneration = df_remuneration[df_remuneration_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only rows in df_remuneration dataframe where director_id occurs in df_ceos dataframe \n",
    "df_remuneration_filter = df_remuneration['director_id'].isin(df_ceos[\"director_id\"])\n",
    "df_remuneration = df_remuneration[df_remuneration_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows with n.a. values in DirectorID or DirectorName\n",
    "# df_remuneration = df_remuneration.dropna(subset=['DirectorName', 'DirectorID'])\n",
    "df_remuneration = df_remuneration[\\\n",
    "    ~(np.isnan(df_remuneration[\"tot_comp\"]))|\\\n",
    "    ~(np.isnan(df_remuneration[\"salary\"]))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new year variable where year corresponds to previous year, i.e. 2018 refers to 2017, etc.\n",
    "df_remuneration[\"year\"] = df_remuneration.date.dt.year + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 'year' variable for merging the dataframes\n",
    "df_ceos[\"year\"] = df_ceos.date.dt.year\n",
    "# _df_remuneration = df_remuneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the dataframe smaller to include only the columns we need\n",
    "df_remuneration = df_remuneration[[\"director_id\", \"company_id\", \"salary\", \"tot_remuneration\", \"bonus\", \"year\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the remuneration dataframe into the ceo dataframe\n",
    "df_ceos.merge(df_remuneration, on=[\"director_id\", \"company_id\", \"year\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Clean up director data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unidentfiable names\n",
    "df_directors = df_directors.iloc[140:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in the nationality and age data\n",
    "df_directors = df_directors.merge(df_ages, on=\"DirectorID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the extra column from the dataframe\n",
    "df_directors = df_directors.drop(['DirectorName_y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename some columns\n",
    "df_directors = df_directors.rename(columns={\n",
    "    'DirectorName_x':'director_name',\\\n",
    "    'CompanyName':'company_name',\\\n",
    "    'BrdPosition':'board_position',\\\n",
    "    'RoleName':'role_name',\\\n",
    "    'NED':'non_exec',\\\n",
    "    'DirectorID':'director_id',\\\n",
    "    'CompanyID':'company_id',\\\n",
    "    'DateStartRole':'start_date',\\\n",
    "    'DateEndRole':'end_date',\\\n",
    "    'HOCountryName':'country',\\\n",
    "    'Sector':'sector',\\\n",
    "    'OrgType':'org_type',\n",
    "    'DOB':'date_of_birth',\\\n",
    "    'Gender':'gender',\\\n",
    "    'Nationality':'nationality',\\\n",
    "    'NetworkSize':'network_size'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_conversion(df):\n",
    "    print(f\"Number of rows before processing: {df.shape[0]}\")\n",
    "    df[\"date_of_birth\"] = [np.nan if (val == 'NaN') | (val == 'n.a.') else val for val in df['date_of_birth']]\n",
    "    df = df.dropna(subset=[\"date_of_birth\"])\n",
    "\n",
    "    regex = r\"(?P<DAY>\\d{2})?[/\\s-]?(?P<MONTH>[a-zA-Z]{3})?[/\\s-]?(?P<YEAR>\\d{4})$\"\n",
    "    df = df.join(df[\"date_of_birth\"].str.extract(regex))\n",
    "\n",
    "    df[\"DAY\"].fillna(\"01\", inplace=True)\n",
    "    df[\"MONTH\"].fillna(\"Jan\", inplace=True)\n",
    "\n",
    "    df[\"date_of_birth\"] = df[\"YEAR\"] + \"-\" + df[\"MONTH\"] + \"-\" + df[\"DAY\"]\n",
    "    df[\"date_of_birth\"] = pd.to_datetime(df[\"date_of_birth\"], format=\"%Y-%b-%d\")\n",
    "\n",
    "    df.drop([\"YEAR\", \"MONTH\", \"DAY\"], axis=1, inplace=True)\n",
    "    print(f\"Number of rows after dropping NAs: {df.shape[0]}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df_directors = date_conversion(df_directors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the entries that are missing StartDate or EndDate\n",
    "df_directors.drop(df_directors[df_directors[\"start_date\"] == \"N\"].index, inplace=True)\n",
    "df_directors.drop(df_directors[df_directors[\"end_date\"] == \"N\"].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Role ID to be used later\n",
    "df_directors['role_id'] = df_directors.index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column to indicate directors who remain in their roles\n",
    "df_directors[\"current_role\"] = False\n",
    "df_directors.loc[df_directors[\"end_date\"] == \"C\",\"current_role\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"C\" DateEndRole to the Maximum End Date in the dataset\n",
    "MaxDate = np.unique(df_directors[\"end_date\"])[-2]\n",
    "df_directors.loc[df_directors[\"end_date\"] == \"C\",\"end_date\"] = MaxDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert start and end date to datetime format\n",
    "df_directors[\"start_date\"] = df_directors[\"start_date\"].astype(np.datetime64)\n",
    "df_directors[\"end_date\"] = df_directors[\"end_date\"].astype(np.datetime64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the role duration. Note that we cannot use this directly, because that is what we're trying to predict.\n",
    "# But this is a known and usable value once a role has ended. If there is a trend, it may continue.\n",
    "df_directors['role_duration'] = df_directors['end_date']-df_directors['start_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/cdeotte/xgb-fraud-with-magic-0-9600#Feature-Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_directors = df_directors.drop([\"YEAR\", \"MONTH\", \"DAY\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate roles (same director_id, role name, same dates, same company), keep the first occurence\n",
    "df_directors.drop_duplicates(subset=[\"director_id\", \"role_name\", \"company_id\", \"start_date\", \"end_date\"], keep=\"first\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt_rows(df):\n",
    "    \"\"\"\n",
    "    Expand the dataframe to melt the start_date and end_date\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.melt(id_vars=['director_name', 'company_name', 'board_position', 'role_name',\n",
    "       'non_exec', 'director_id', 'company_id', 'country', 'sector', \n",
    "       'org_type', 'ISIN', 'date_of_birth', 'gender',\n",
    "       'nationality', 'network_size', 'role_id', 'current_role',\n",
    "       'role_duration'],value_name='date',var_name='date_type')\n",
    "\n",
    "    df = df.drop_duplicates(subset=['role_id', 'date'], keep=False)\n",
    "    \n",
    "    # df = df.groupby('RoleId').apply(lambda x: x.set_index('Date').resample('M').pad())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_directors = melt_rows(df_directors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift the role duration so that it is only at the end of the role that we start counting completed roles\n",
    "df_directors['past_role_duration'] = df_directors.groupby(['role_id'])['role_duration'].shift(1)\n",
    "df_directors.loc[pd.isnull(df_directors['past_role_duration']), 'past_role_duration'] = np.timedelta64(0, \"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Sum previous role duration to derive career length and company tenure\n",
    "#  Sort end_date < start_date to ensure we capture tenure for subsequent roles where start_date matches previous role end_date\n",
    "df_directors.sort_values(['director_id', 'date', 'date_type'], ascending=[True,True,True], inplace = True)\n",
    "df_directors['past_roles_tenure'] = df_directors.groupby('director_id')['past_role_duration'].transform(pd.Series.cumsum)\n",
    "df_directors['company_roles_tenure'] = df_directors.groupby(['director_id', 'company_id'])['past_role_duration'].transform(pd.Series.cumsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the roles that last <1 day\n",
    "df_directors = df_directors.drop_duplicates(subset=['role_id', 'date'], keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a PastRolesIncrementer as well as an ActiveRolesIncrementer to be used to tally the sum of past and active roles\n",
    "df_directors['past_roles_incrementer'] = [0 if DateType == 'start_date' else 1 for DateType in df_directors['date_type']]\n",
    "df_directors['active_roles_incrementer'] = [1 if val == 'start_date' else -1 for val in df_directors['date_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of previous roles career-wide\n",
    "df_directors.sort_values(['director_id', 'date', 'date_type'], ascending=[True,True,True], inplace = True)\n",
    "df_directors['past_roles_count'] = df_directors.groupby(['director_id'])['past_roles_incrementer'].transform(pd.Series.cumsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of previous roles at the company\n",
    "# df_directors.sort_values(['director_id', 'date', 'date_type'], ascending=[True,True,True], inplace = True)\n",
    "df_directors['company_roles_count'] = df_directors.groupby(['director_id', 'company_id'])['past_roles_incrementer'].transform(pd.Series.cumsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average tenure of past roles\n",
    "df_directors[\"past_roles_tenure_avg\"] = df_directors[\"past_roles_tenure\"]/df_directors[\"past_roles_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average tenure of past roles at the company\n",
    "df_directors[\"company_roles_tenure_avg\"] = df_directors[\"company_roles_tenure\"]/df_directors[\"company_roles_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join roles where the same director, keeps the same role, same role_name, at the same company\n",
    "df_directors.drop_duplicates(subset=[\"director_id\", \"role_name\", \"company_id\", \"date\"], keep=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In each month, take the sum of all active roles increments to calculate net role changes\n",
    "df_directors['active_roles_incrementer'] = df_directors.groupby(['director_id', 'date'])['active_roles_incrementer'].transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to identify some duplicate months for each director (without dropping them)\n",
    "df_directors['duplicata'] = df_directors.duplicated(subset=['director_id', 'date'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert the duplicate tag into a value of 0 if it's true, so that we can calculate active roles\n",
    "df_directors['duplicata'] = [0 if val else 1 for val in df_directors['duplicata']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate active_roles_incrementer without duplicates\n",
    "df_directors['active_roles_inc_no_dup'] = df_directors['active_roles_incrementer']*df_directors['duplicata'] \n",
    "df_directors['active_roles_count'] = df_directors.groupby('director_id')['active_roles_inc_no_dup'].transform('cumsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the maximum number of active roles on a rolling basis\n",
    "df_directors['active_roles_count_max'] = df_directors.groupby('director_id')['active_roles_count'].cummax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create a CEO-only dataset with expanded rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an indicator variable for CEOs, Chairmen and CEOs who also hold the title of Chair\n",
    "df_directors[\"ceo\"] = df_directors[\"role_name\"].str.contains('ceo', case=False, regex=False)\n",
    "df_directors[\"chair\"] = df_directors[\"role_name\"].str.contains('chairman', case=False, regex=False) \n",
    "df_directors[\"chair_ceo\"] = (df_directors[\"chair\"]) & (df_directors[\"ceo\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe filtered to CEOs\n",
    "df_ceos = df_directors[df_directors[\"ceo\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Regional, Division, Acting, Interim CEOs from dataframe\n",
    "df_ceos = df_ceos[~df_ceos[\"role_name\"].str.contains('Division|Interim|Acting|Regional|Deputy\\sCEO', regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the tenure of the previous CEO\n",
    "df_ceos.sort_values(['company_id', 'date', 'date_type'], ascending=[True,True,True], inplace = True)\n",
    "df_ceos[\"previous_role_duration\"] = df_ceos.groupby(['role_id'])['role_duration'].shift(1)\n",
    "df_ceos['previous_ceo_tenure'] = df_ceos.groupby(['company_id'])['previous_role_duration'].ffill()\n",
    "df_ceos['previous_ceo_tenure'] = df_ceos.groupby(['company_id'])['previous_ceo_tenure'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify role extensions, i.e. where a CEO continues in his/her role\n",
    "df_ceos[\"role_extension\"] = df_ceos.duplicated(subset=[\"director_id\", \"company_id\", \"date\"], keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where our estimated date is wrong (role_duration is a negative number)\n",
    "drop_this = (df_ceos[\"role_duration\"]<np.timedelta64(0, \"Y\"))\n",
    "df_ceos = df_ceos[~drop_this]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in the gvkey to the directors dataframe \n",
    "df_lookup_table = df_lookup_table.rename(columns={'companyid':'company_id'})\n",
    "df_ceos = df_ceos.merge(df_lookup_table, on='company_id', how='left')\n",
    "df_ceos = df_ceos.rename(columns={'GVKEY':'gvkey'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out which gvkey in the directors dataset appears in the performance dataset\n",
    "# Select only rows in df_ceos dataframe that co-occur in share prices dataframe (i.e. publicly-traded companies)\n",
    "gvkey_list = df_ceos.gvkey.isin(df_share_prices[\"gvkey\"])\n",
    "df_ceos = df_ceos[gvkey_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional step: downsample some of the people appearing in the dataset very often\n",
    "df_ceos.groupby('director_id').apply(lambda x: x.sample(frac=0.2) if (len(x)>50) else x).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand the rows of the dataset so that one row corresponds to one month\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def parallelize_dataframe(df, func, n_cores=8):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "    \n",
    "def expand_rows(df):\n",
    "    df = df.groupby('role_id').apply(lambda x: x.set_index('date').resample('M').pad())\n",
    "    df = df.droplevel(level=0, axis=0)\n",
    "    df.reset_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "df_ceos = parallelize_dataframe(df_ceos, expand_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add back end_date and start_date columns \n",
    "df_ceos[\"end_date\"] = df_ceos.groupby('role_id')['date'].transform('max')\n",
    "df_ceos[\"start_date\"] = df_ceos.groupby('role_id')['date'].transform('min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add role_tenure\n",
    "df_ceos[\"role_tenure\"] = df_ceos[\"date\"] - df_ceos[\"start_date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a response variable set to TRUE if a turnover event takes place within the next 12 months\n",
    "df_ceos[\"time_left\"] = df_ceos[\"end_date\"] - df_ceos[\"date\"]\n",
    "df_ceos[\"turnover\"] = (df_ceos['time_left'] < np.timedelta64(1, \"Y\")) & (df_ceos['role_extension']==False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop roles from the dataframe that started before 1990 (more than 30 years ago)\n",
    "expired_data = (df_ceos[\"start_date\"].dt.year < 1990)\n",
    "df_ceos = df_ceos[~expired_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where there is a turnover event but it's a current role\n",
    "df_ceos = df_ceos[~((df_ceos.turnover==True) & (df_ceos.current_role==True))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only necessary columns \n",
    "df_ceos = df_ceos[['date', 'director_name',\\\n",
    " 'company_name','role_name','director_id',\\\n",
    "  'company_id', 'date_of_birth','gender', \\\n",
    "'nationality','network_size', 'role_id', \\\n",
    "'current_role', 'role_duration','past_roles_count',\\\n",
    "'company_roles_count', 'past_roles_tenure_avg',\\\n",
    "'company_roles_tenure_avg', 'ceo', 'chair', \\\n",
    "'chair_ceo', 'past_role_duration', 'past_roles_tenure',\\\n",
    " 'company_roles_tenure','active_roles_count',\\\n",
    " 'active_roles_count_max','previous_ceo_tenure', \\\n",
    " 'role_extension','end_date', 'start_date','role_tenure','turnover']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add CEO age \n",
    "df_ceos['age'] = (df_ceos['date'] - df_ceos['date_of_birth'])/np.timedelta64(1,'Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Merge the company and share price datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge in performance dataframe into the ceo dataframe\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def parallel_merge(df, func, n_cores=8):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "    \n",
    "def merge_dataframes(df):\n",
    "    df.merge(df_share_prices, on=['date', 'gvkey'], validate='many_to_one')\n",
    "    return df\n",
    "\n",
    "df_ceos = parallel_merge(df_ceos, merge_dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Data preparation: dealing with NAs, encoding and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get names of columns with missing values\n",
    "def identify_missing(df):\n",
    "    list = []\n",
    "    # Ignore return columns (labeled 'rtn') for now \n",
    "    list = [col for col in df.columns if df[col].isnull().any()]\n",
    "    return list\n",
    "\n",
    "cols_with_missing = identify_missing(df_ceos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the first 36 months of the dataset\n",
    "df_ceos = df_ceos[df_ceos[\"date\"] > \"01-01-1993\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate rows that have columns with missing data\n",
    "for col in cols_with_missing:\n",
    "    df_ceos[col + '_was_missing'] = df_ceos[col].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to CSV file\n",
    "df_ceos.to_csv(\"./data/csv_files/ceos.csv.gz\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitmlturnovervenvc063555a201c4c1785c4dfc0338511ad",
   "display_name": "Python 3.7.7 64-bit ('ml-turnover': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
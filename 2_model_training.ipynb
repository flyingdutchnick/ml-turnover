{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CEO Turnover Prediction using XGBoost\n",
    "_**Using Gradient Boosted Trees to Predict the departures of CEOs**_\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Compile](#Compile)\n",
    "1. [Host](#Host)\n",
    "  1. [Evaluate](#Evaluate)\n",
    "  1. [Relative cost of errors](#Relative-cost-of-errors)\n",
    "1. [Next Steps](#Next-Steps)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "_This notebook has been adapted from a [blog post](https://aws.amazon.com/blogs/ai/predicting-customer-churn-with-amazon-machine-learning/)_\n",
    "\n",
    "In most companies, the top role is that of the Chief Executive Officer (CEO). The CEO role is a tough one to fill. In 2018, about 30% of the CEO departures in the Fortune 500 were involuntary, according to the Conference Board. Hiring and firing of a CEO is the responsibility of the board of a company, typically supported by a consultant. \n",
    "\n",
    "If the board could identify CEOs that are at risk of leaving early on, they could plan ahead. Stakeholders might care to know. A vacant CEO position is typically frowned upon by the investment community and viewed as a failure of the board to carry out on of its most important duties, that of appointing a CEO. \n",
    "\n",
    "This notebook is an experiment to use machine learning (ML) for the automated identification of CEO departures or attrition, also referred to in the academic literature as turnovers. ML models do not give perfect predictions, though, so this notebook is also about how to incorporate the relative costs of prediction mistakes when determining an outcome of using ML. \n",
    "\n",
    "For our model, we collected data on CEOs in North America from the past 20 years. While academic models make a distinction between forced departures and voluntary departures, this model does not follow that approach. For the purposes of this exercise, a turnover event is described as one taking place within the next 12 months. In other words, in the 11 months preceding the actual departure of the CEO, we should mark the turnover variable as **TRUE**.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.m4.xlarge notebook instance._\n",
    "\n",
    "We start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "isConfigCell": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket = 'sagemaker-us-east-1-936165954724'\n",
    "prefix = 'ml-turnover'\n",
    "\n",
    "# Define IAM role\n",
    "import awscli\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll import the Python libraries we'll need for the remainder of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas.api.types as ptypes\n",
    "\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from IPython.display import display\n",
    "from time import strftime, gmtime\n",
    "import sagemaker\n",
    "from sagemaker.predictor import csv_serializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data\n",
    "\n",
    "In this experiment, we are limited to public data (others m). Therefore, by design, it will be a relaaweak model.  \n",
    "\n",
    "historical records on which customers ultimately ended up churning and which continued using the service. We can use this historical information to construct an ML model of one mobile operator’s churn using a process called training. After training the model, we can pass the profile information of an arbitrary customer (the same profile information that we used to train the model) to the model, and have the model predict whether this customer is going to churn. Of course, we expect the model to make mistakes–after all, predicting the future is tricky business! But I’ll also show how to deal with prediction errors.\n",
    "\n",
    "The dataset we use is publicly available and was mentioned in the book [Discovering Knowledge in Data](https://www.amazon.com/dp/0470908742/) by Daniel T. Larose. It is attributed by the author to the University of California Irvine Repository of Machine Learning Datasets.  Let's download and read that dataset in now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget [TBD]\n",
    "!unzip -o DKD2e_data_sets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ceos = pd.read_csv('./data/csv_files/ceos.csv.gz', compression='gzip')\n",
    "pd.set_option('display.max_columns', 500)\n",
    "df_ceos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to CSV file\n",
    "df_ceos.to_csv(\"./data/csv_files/ceos.csv.gz\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By modern standards, it’s a relatively small dataset, with only 3,333 records, where each record uses 21 attributes to describe the profile of a customer of an unknown US mobile operator. The attributes are:\n",
    "\n",
    "- `State`: the US state in which the customer resides, indicated by a two-letter abbreviation; for example, OH or NJ\n",
    "- `Account Length`: the number of days that this account has been active\n",
    "- `Area Code`: the three-digit area code of the corresponding customer’s phone number\n",
    "- `Phone`: the remaining seven-digit phone number\n",
    "- `Int’l Plan`: whether the customer has an international calling plan: yes/no\n",
    "- `VMail Plan`: whether the customer has a voice mail feature: yes/no\n",
    "- `VMail Message`: presumably the average number of voice mail messages per month\n",
    "- `Day Mins`: the total number of calling minutes used during the day\n",
    "- `Day Calls`: the total number of calls placed during the day\n",
    "- `Day Charge`: the billed cost of daytime calls\n",
    "- `Eve Mins, Eve Calls, Eve Charge`: the billed cost for calls placed during the evening\n",
    "- `Night Mins`, `Night Calls`, `Night Charge`: the billed cost for calls placed during nighttime\n",
    "- `Intl Mins`, `Intl Calls`, `Intl Charge`: the billed cost for international calls\n",
    "- `CustServ Calls`: the number of calls placed to Customer Service\n",
    "- `Churn?`: whether the customer left the service: true/false\n",
    "\n",
    "The last attribute, `Churn?`, is known as the target attribute–the attribute that we want the ML model to predict.  Because the target attribute is binary, our model will be performing binary prediction, also known as binary classification.\n",
    "\n",
    "Let's begin exploring the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['nationality',\n 'network_size',\n 'past_roles_tenure_avg',\n 'company_roles_tenure_avg',\n 'previous_ceo_tenure',\n 'firm_rtn_1m',\n 'firm_rtn_3m',\n 'firm_rtn_6m',\n 'firm_rtn_12m',\n 'firm_rtn_24m',\n 'firm_rtn_36m',\n 'sic',\n 'sector_rtn_1m',\n 'sector_rtn_3m',\n 'sector_rtn_6m',\n 'sector_rtn_12m',\n 'sector_rtn_24m',\n 'sector_rtn_36m']"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "# Get names of columns with missing values\n",
    "def identify_missing(df):\n",
    "    list = []\n",
    "    # Ignore return columns (labeled 'rtn') for now \n",
    "    list = [col for col in df.columns if df[col].isnull().any()]\n",
    "    return list\n",
    "\n",
    "cols_with_missing = identify_missing(df_ceos)\n",
    "cols_with_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, test_data = np.split(df_ceos.sample(frac=1, random_state=1024), [int(0.7 * len(df_ceos)), int(0.9 * len(df_ceos))])\n",
    "X_train.to_csv('./data/training/train.csv', header=True, index=False)\n",
    "X_valid.to_csv('./data//validation.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols_with_missing:\n",
    "    df_ceos[col + '_was_missing'] = df_ceos[col].isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Imput missing NA values using a simple imputer (mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      0    1    2    3    4\n34  170  171  172  173  174\n19   95   96   97   98   99\n26  130  131  132  133  134\n6    30   31   32   33   34\n31  155  156  157  158  159\n1     5    6    7    8    9",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>34</th>\n      <td>170</td>\n      <td>171</td>\n      <td>172</td>\n      <td>173</td>\n      <td>174</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>95</td>\n      <td>96</td>\n      <td>97</td>\n      <td>98</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>130</td>\n      <td>131</td>\n      <td>132</td>\n      <td>133</td>\n      <td>134</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>30</td>\n      <td>31</td>\n      <td>32</td>\n      <td>33</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>155</td>\n      <td>156</td>\n      <td>157</td>\n      <td>158</td>\n      <td>159</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>6</td>\n      <td>7</td>\n      <td>8</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataframe\n",
    "def get_data_splits(df, test_fraction=0.15):\n",
    "    \"\"\"Splits a dataframe into train, validation, and test sets.\n",
    "\n",
    "    First, orders by the column 'click_time'. Set the size of the \n",
    "    validation and test sets with the valid_fraction keyword argument.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.sample(frac=1, random_state=1776)\n",
    "    \n",
    "    train_set, valid_set, test_set = np.array_split(df, [int((1-2*test_fraction)*len(df)), int((1-test_fraction)*len(df))])\n",
    "\n",
    "    # a_train_set, a_test_set = train_test_split(a, train_size=0.85, test_fraction=0.15, random_state=10)\n",
    "    # a_train_set, a_validation_set = train_test_split(a_train_set, train_size=0.85, test_fraction=0.15, random_state=10)\n",
    "    # print(pd.concat([a_test_set, a_validation_set, a_train_set]))\n",
    "    \n",
    "    return train_set, valid_set, test_set\n",
    "\n",
    "a_train_set, a_valid_set, a_test_set = get_data_splits(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "#           COLUMNS BY TYPE      #\n",
    "##################################\n",
    "\n",
    "# Categorical features\n",
    "cat_features = ['director_id', 'company_id',\\\n",
    "                'role_name','gender','nationality', \\\n",
    "                'role_id', 'sic']\n",
    "                \n",
    "\n",
    "# Drop columns\n",
    "drop_cols = ['director_name', 'company_name','gvkey','tic'\\\n",
    "            'end_date', 'current_role', 'role_duration']\n",
    "\n",
    "# Date features\n",
    "date_features = ['start_date', 'date', 'date_of_birth']\n",
    "\n",
    "# Timedelta features:\n",
    "timedelta_features = ['company_roles_tenure','company_roles_tenure_avg','past_role_duration',\\\n",
    "                    'past_roles_tenure','past_roles_tenure_avg','previous_ceo_tenure',\\\n",
    "                    'role_tenure','tenure_at_ceo_start'\n",
    "                    ]\n",
    "# Numerical features\n",
    "num_features = ['network_size','past_roles_count','age', \\\n",
    "                'company_roles_count','active_roles_count','active_roles_count_max',\\\n",
    "                'firm_rtn_1m', 'firm_rtn_3m', 'firm_rtn_6m',\\\n",
    "                'firm_rtn_12m', 'firm_rtn_24m', 'firm_rtn_36m',\\\n",
    "                'index_rtn_1m', 'index_rtn_3m', 'index_rtn_6m',\\\n",
    "                'index_rtn_12m', 'index_rtn_24m', 'index_rtn_36m',\\\n",
    "                'sector_rtn_1m', 'sector_rtn_3m', 'sector_rtn_6m',\\\n",
    "                'sector_rtn_12m','sector_rtn_24m', 'sector_rtn_36m'\n",
    "                ]\n",
    "\n",
    "# Boolean variables\n",
    "bool_features = ['role_extension','nationality_was_missing',\\\n",
    "       'network_size_was_missing', 'past_roles_tenure_avg_was_missing',\\\n",
    "       'company_roles_tenure_avg_was_missing','ceo','chair', 'chair_ceo',\\\n",
    "       'previous_ceo_tenure_was_missing', 'firm_rtn_1m_was_missing',\\\n",
    "       'firm_rtn_3m_was_missing', 'firm_rtn_6m_was_missing',\\\n",
    "       'firm_rtn_12m_was_missing', 'firm_rtn_24m_was_missing',\\\n",
    "       'firm_rtn_36m_was_missing', 'sic_was_missing',\\\n",
    "       'sector_rtn_1m_was_missing', 'sector_rtn_3m_was_missing',\\\n",
    "       'sector_rtn_6m_was_missing', 'sector_rtn_12m_was_missing',\\\n",
    "       'sector_rtn_24m_was_missing', 'sector_rtn_36m_was_missing'\n",
    "]\n",
    "\n",
    "# Outcome variable\n",
    "y_var = ['turnover']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time features to their corresponding data type (lost when saved as csv)\n",
    "\n",
    "df_ceos[timedelta_features] = df_ceos[timedelta_features].apply(pd.to_timedelta)\n",
    "df_ceos[cat_features] = df_ceos[cat_features].astype('object')\n",
    "df_ceos[date_features] = df_ceos[date_features].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Test column types:\n\n##### 1/5 Passed:\n\ndirector_id    object\ncompany_id     object\nrole_name      object\ngender         object\nnationality    object\nrole_id        object\nsic            object\ndtype: object\n\n##### 2/5 Passed:\n\ncompany_roles_tenure        timedelta64[ns]\ncompany_roles_tenure_avg    timedelta64[ns]\npast_role_duration          timedelta64[ns]\npast_roles_tenure           timedelta64[ns]\npast_roles_tenure_avg       timedelta64[ns]\nprevious_ceo_tenure         timedelta64[ns]\nrole_tenure                 timedelta64[ns]\ntenure_at_ceo_start         timedelta64[ns]\ndtype: object\n\n##### 3/5 Passed:\n\nstart_date       datetime64[ns]\ndate             datetime64[ns]\ndate_of_birth    datetime64[ns]\ndtype: object\n\n##### 4/5 Passed:\n\nnetwork_size              float64\npast_roles_count            int64\nage                       float64\ncompany_roles_count         int64\nactive_roles_count          int64\nactive_roles_count_max      int64\nfirm_rtn_1m               float64\nfirm_rtn_3m               float64\nfirm_rtn_6m               float64\nfirm_rtn_12m              float64\nfirm_rtn_24m              float64\nfirm_rtn_36m              float64\nindex_rtn_1m              float64\nindex_rtn_3m              float64\nindex_rtn_6m              float64\nindex_rtn_12m             float64\nindex_rtn_24m             float64\nindex_rtn_36m             float64\nsector_rtn_1m             float64\nsector_rtn_3m             float64\nsector_rtn_6m             float64\nsector_rtn_12m            float64\nsector_rtn_24m            float64\nsector_rtn_36m            float64\ndtype: object\n\n##### 5/5 Passed:\n\nrole_extension                          bool\nnationality_was_missing                 bool\nnetwork_size_was_missing                bool\npast_roles_tenure_avg_was_missing       bool\ncompany_roles_tenure_avg_was_missing    bool\nceo                                     bool\nchair                                   bool\nchair_ceo                               bool\nprevious_ceo_tenure_was_missing         bool\nfirm_rtn_1m_was_missing                 bool\nfirm_rtn_3m_was_missing                 bool\nfirm_rtn_6m_was_missing                 bool\nfirm_rtn_12m_was_missing                bool\nfirm_rtn_24m_was_missing                bool\nfirm_rtn_36m_was_missing                bool\nsic_was_missing                         bool\nsector_rtn_1m_was_missing               bool\nsector_rtn_3m_was_missing               bool\nsector_rtn_6m_was_missing               bool\nsector_rtn_12m_was_missing              bool\nsector_rtn_24m_was_missing              bool\nsector_rtn_36m_was_missing              bool\ndtype: object\n"
    }
   ],
   "source": [
    "# Assert all data types are what they are supposed to be\n",
    "\n",
    "def assert_col_types(df):\n",
    "    print('Test column types:')\n",
    "    try: \n",
    "        assert all(ptypes.is_object_dtype(df[col]) for col in cat_features)\n",
    "        print(f'\\n##### 1/5 Passed:\\n')\n",
    "        print(f'{df[cat_features].dtypes}')\n",
    "        assert all(ptypes.is_timedelta64_dtype(df[col]) for col in timedelta_features)\n",
    "        print(f'\\n##### 2/5 Passed:\\n')\n",
    "        print(f'{df[timedelta_features].dtypes}')\n",
    "        assert all(ptypes.is_datetime64_dtype(df[col]) for col in date_features)\n",
    "        print(f'\\n##### 3/5 Passed:\\n')\n",
    "        print(f'{df[date_features].dtypes}')\n",
    "        assert all((ptypes.is_int64_dtype(df[col])|ptypes.is_float_dtype(df[col])) for col in num_features)\n",
    "        print(f'\\n##### 4/5 Passed:\\n')\n",
    "        print(f'{df[num_features].dtypes}')\n",
    "        assert all(ptypes.is_bool_dtype(df[col]) for col in bool_features)\n",
    "        print(f'\\n##### 5/5 Passed:\\n')\n",
    "        print(f'{df[bool_features].dtypes}')\n",
    "    except: \n",
    "        print('\\nOne or more tests failed')\n",
    "    \n",
    "assert_col_types(df_ceos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with some missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.003556095692262982"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "# Break off validation set from training data\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n",
    "                                                                train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1135515"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "# Split into training, validation, test sets\n",
    "df_ceos.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "numerical_missing_cols = ['firm_rtn_1m','firm_rtn_3m','firm_rtn_6m',\\\n",
    "            'firm_rtn_12m','firm_rtn_24m','firm_rtn_36m',\\\n",
    "            'sector_rtn_1m','sector_rtn_3m','sector_rtn_6m',\\\n",
    "            'sector_rtn_12m','sector_rtn_24m','sector_rtn_36m']\n",
    "\n",
    "### Add some 'missing' and zeroes to certain columns with missing data \n",
    "\n",
    "df_ceos['nationality'].fillna(\"missing\", inplace=True)\n",
    "df_ceos['sic'].fillna(\"missing\", inplace=True)\n",
    "df_ceos['past_roles_tenure_avg'].fillna(0, inplace=True)\n",
    "df_ceos['company_roles_tenure_avg'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "df_ceos['previous_ceo_tenure'].fillna(, inplace=True)\n",
    "df_ceos['network_size'].fillna('mean', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Apply one-hot encoding to cols with categorical data\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
    "\n",
    "###  Add the averages to the numerical cols\n",
    "\n",
    "df['previous_ceo_tenure'].fillna(df['previous_ceo_tenure'].mean(), inplace=True)\n",
    "df['previous_ceo_tenure'].fillna(df['previous_ceo_tenure'].mean(), inplace=True)\n",
    "df['previous_ceo_tenure'].fillna(df['previous_ceo_tenure'].mean(), inplace=True)\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train_plus.columns = X_train_plus.columns\n",
    "imputed_X_valid_plus.columns = X_valid_plus.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "              date director_name  \\\n0       2018-09-30   A Beharelle   \n1       2018-09-30  Steve Cooper   \n2       2018-10-31   A Beharelle   \n3       2018-11-30   A Beharelle   \n4       2018-12-31   A Beharelle   \n...            ...           ...   \n1135510 2018-12-31  Zander Lurie   \n1135511 2019-01-31  Zander Lurie   \n1135512 2019-02-28  Zander Lurie   \n1135513 2019-03-31  Zander Lurie   \n1135514 2019-04-30  Zander Lurie   \n\n                                            company_name      role_name  \\\n0        TRUEBLUE INC (Labor Ready Inc prior to 12/2007)  President/CEO   \n1        TRUEBLUE INC (Labor Ready Inc prior to 12/2007)            CEO   \n2        TRUEBLUE INC (Labor Ready Inc prior to 12/2007)  President/CEO   \n3        TRUEBLUE INC (Labor Ready Inc prior to 12/2007)  President/CEO   \n4        TRUEBLUE INC (Labor Ready Inc prior to 12/2007)  President/CEO   \n...                                                  ...            ...   \n1135510                          SVMK INC (SurveyMonkey)            CEO   \n1135511                          SVMK INC (SurveyMonkey)            CEO   \n1135512                          SVMK INC (SurveyMonkey)            CEO   \n1135513                          SVMK INC (SurveyMonkey)            CEO   \n1135514                          SVMK INC (SurveyMonkey)            CEO   \n\n        director_id company_id date_of_birth gender nationality  network_size  \\\n0           1340849      18329    1969-01-01      M         NaN        2669.0   \n1            274662      18329    1962-01-01      M    American         721.0   \n2           1340849      18329    1969-01-01      M         NaN        2669.0   \n3           1340849      18329    1969-01-01      M         NaN        2669.0   \n4           1340849      18329    1969-01-01      M         NaN        2669.0   \n...             ...        ...           ...    ...         ...           ...   \n1135510      549856    2920393    1974-01-01      M         NaN        1453.0   \n1135511      549856    2920393    1974-01-01      M         NaN        1453.0   \n1135512      549856    2920393    1974-01-01      M         NaN        1453.0   \n1135513      549856    2920393    1974-01-01      M         NaN        1453.0   \n1135514      549856    2920393    1974-01-01      M         NaN        1453.0   \n\n         role_id  current_role                 role_duration  \\\n0            218          True   608 days 00:00:00.000000000   \n1        5240630         False  1206 days 00:00:00.000000000   \n2            218          True   608 days 00:00:00.000000000   \n3            218          True   608 days 00:00:00.000000000   \n4            218          True   608 days 00:00:00.000000000   \n...          ...           ...                           ...   \n1135510  5833307         False   187 days 00:00:00.000000000   \n1135511  5833307         False   187 days 00:00:00.000000000   \n1135512  5833307         False   187 days 00:00:00.000000000   \n1135513  5833307         False   187 days 00:00:00.000000000   \n1135514  5833307         False   187 days 00:00:00.000000000   \n\n         past_roles_count  company_roles_count     past_roles_tenure_avg  \\\n0                       7                    2 1381 days 20:34:17.142857   \n1                      10                    7        1229 days 14:24:00   \n2                       7                    2 1381 days 20:34:17.142857   \n3                       7                    2 1381 days 20:34:17.142857   \n4                       7                    2 1381 days 20:34:17.142857   \n...                   ...                  ...                       ...   \n1135510                19                    0  754 days 07:34:44.210526   \n1135511                19                    0  754 days 07:34:44.210526   \n1135512                19                    0  754 days 07:34:44.210526   \n1135513                19                    0  754 days 07:34:44.210526   \n1135514                20                    1         725 days 22:48:00   \n\n         company_roles_tenure_avg   ceo  chair  chair_ceo past_role_duration  \\\n0               762 days 00:00:00  True  False      False             0 days   \n1       1013 days 06:51:25.714285  True  False      False          1206 days   \n2               762 days 00:00:00  True  False      False             0 days   \n3               762 days 00:00:00  True  False      False             0 days   \n4               762 days 00:00:00  True  False      False             0 days   \n...                           ...   ...    ...        ...                ...   \n1135510                       NaT  True  False      False             0 days   \n1135511                       NaT  True  False      False             0 days   \n1135512                       NaT  True  False      False             0 days   \n1135513                       NaT  True  False      False             0 days   \n1135514         187 days 00:00:00  True  False      False           187 days   \n\n        past_roles_tenure company_roles_tenure  active_roles_count  \\\n0               9673 days            1524 days                   3   \n1              12296 days            7093 days                   3   \n2               9673 days            1524 days                   3   \n3               9673 days            1524 days                   3   \n4               9673 days            1524 days                   3   \n...                   ...                  ...                 ...   \n1135510        14332 days               0 days                   3   \n1135511        14332 days               0 days                   3   \n1135512        14332 days               0 days                   3   \n1135513        14332 days               0 days                   3   \n1135514        14519 days             187 days                   3   \n\n         active_roles_count_max previous_ceo_tenure  role_extension  \\\n0                             3           1206 days           False   \n1                             3           3284 days           False   \n2                             3           1206 days           False   \n3                             3           1206 days           False   \n4                             3           1206 days           False   \n...                         ...                 ...             ...   \n1135510                       5                 NaT           False   \n1135511                       5                 NaT           False   \n1135512                       5                 NaT           False   \n1135513                       5                 NaT           False   \n1135514                       5                 NaT            True   \n\n           end_date start_date role_tenure  turnover    gvkey   tic  \\\n0        2020-05-31 2018-09-30      0 days     False  22207.0   TBI   \n1        2018-09-30 2015-05-31   1218 days      True  22207.0   TBI   \n2        2020-05-31 2018-09-30     31 days     False  22207.0   TBI   \n3        2020-05-31 2018-09-30     61 days     False  22207.0   TBI   \n4        2020-05-31 2018-09-30     92 days     False  22207.0   TBI   \n...             ...        ...         ...       ...      ...   ...   \n1135510  2019-04-30 2018-09-30     92 days      True  34140.0  SVMK   \n1135511  2019-04-30 2018-09-30    123 days      True  34140.0  SVMK   \n1135512  2019-04-30 2018-09-30    151 days      True  34140.0  SVMK   \n1135513  2019-04-30 2018-09-30    182 days      True  34140.0  SVMK   \n1135514  2019-04-30 2018-09-30    212 days     False  34140.0  SVMK   \n\n         firm_rtn_1m  firm_rtn_3m  firm_rtn_6m  firm_rtn_12m  firm_rtn_24m  \\\n0          -0.110922    -0.033395     0.005792      0.160356      0.149603   \n1          -0.110922    -0.033395     0.005792      0.160356      0.149603   \n2          -0.104415    -0.137523    -0.124578     -0.139114      0.333143   \n3           0.082297    -0.138225    -0.021318     -0.112478      0.205251   \n4          -0.118812    -0.145873    -0.174397     -0.190909     -0.097363   \n...              ...          ...          ...           ...           ...   \n1135510    -0.140756    -0.234560          NaN           NaN           NaN   \n1135511     0.060310     0.214753          NaN           NaN           NaN   \n1135512     0.166026     0.062325          NaN           NaN           NaN   \n1135513     0.200396     0.484108     0.135995           NaN           NaN   \n1135514    -0.017024     0.375865     0.671335           NaN           NaN   \n\n         firm_rtn_36m  index_rtn_1m  index_rtn_3m  index_rtn_6m  \\\n0            0.159324      0.000250      0.066419      0.102813   \n1            0.159324      0.000250      0.066419      0.102813   \n2           -0.194684     -0.074608     -0.043837      0.017711   \n3           -0.137931      0.017702     -0.057991      0.009514   \n4           -0.136258     -0.094623     -0.147341     -0.090708   \n...               ...           ...           ...           ...   \n1135510           NaN     -0.094623     -0.147341     -0.090708   \n1135511           NaN      0.084507     -0.000732     -0.044537   \n1135512           NaN      0.032998      0.014288     -0.044532   \n1135513           NaN      0.013028      0.134888     -0.032327   \n1135514           NaN      0.038843      0.087103      0.086307   \n\n         index_rtn_12m  index_rtn_24m  index_rtn_36m        age  \\\n0             0.154530       0.344012       0.513402  49.745032   \n1             0.154530       0.344012       0.513402  56.745861   \n2             0.046658       0.272652       0.299454  49.829908   \n3             0.036178       0.242836       0.318132  49.912045   \n4            -0.069899       0.105442       0.220573  49.996920   \n...                ...            ...            ...        ...   \n1135510      -0.069899       0.105442       0.220573  44.997502   \n1135511      -0.040934       0.177848       0.404371  45.082377   \n1135512       0.030671       0.175642       0.454904  45.159038   \n1135513       0.067163       0.192087       0.379414  45.243913   \n1135514       0.105547       0.226712       0.425706  45.326051   \n\n        tenure_at_ceo_start   sic  sector_rtn_1m  sector_rtn_3m  \\\n0                 1524 days  7363      -0.063492       0.050542   \n1                 2603 days  7363      -0.063492       0.050542   \n2                 1524 days  7363      -0.084437      -0.139710   \n3                 1524 days  7363      -0.023455      -0.167763   \n4                 1524 days  7363      -0.129275      -0.226301   \n...                     ...   ...            ...            ...   \n1135510              0 days  7370      -0.063501      -0.160718   \n1135511              0 days  7370       0.163315       0.098375   \n1135512              0 days  7370       0.085447       0.180294   \n1135513              0 days  7370       0.006231       0.270859   \n1135514              0 days  7370       0.038043       0.139584   \n\n         sector_rtn_6m  sector_rtn_12m  sector_rtn_24m  sector_rtn_36m  \\\n0             0.088124        0.245874        0.334664        0.460395   \n1             0.088124        0.245874        0.334664        0.460395   \n2             0.017290        0.064980        0.300540        0.284561   \n3            -0.058713       -0.043130        0.155166        0.253644   \n4            -0.194127       -0.135414       -0.031325        0.091186   \n...                ...             ...             ...             ...   \n1135510      -0.125409        0.154433        0.520593        0.690130   \n1135511       0.028457        0.248292        0.647609        1.269113   \n1135512       0.008292        0.267708        0.791593        1.471535   \n1135513       0.038207        0.250032        0.794134        1.228860   \n1135514       0.228622        0.275216        0.741504        1.237217   \n\n         nationality_was_missing  network_size_was_missing  \\\n0                           True                     False   \n1                          False                     False   \n2                           True                     False   \n3                           True                     False   \n4                           True                     False   \n...                          ...                       ...   \n1135510                     True                     False   \n1135511                     True                     False   \n1135512                     True                     False   \n1135513                     True                     False   \n1135514                     True                     False   \n\n         past_roles_tenure_avg_was_missing  \\\n0                                    False   \n1                                    False   \n2                                    False   \n3                                    False   \n4                                    False   \n...                                    ...   \n1135510                              False   \n1135511                              False   \n1135512                              False   \n1135513                              False   \n1135514                              False   \n\n         company_roles_tenure_avg_was_missing  \\\n0                                       False   \n1                                       False   \n2                                       False   \n3                                       False   \n4                                       False   \n...                                       ...   \n1135510                                  True   \n1135511                                  True   \n1135512                                  True   \n1135513                                  True   \n1135514                                 False   \n\n         previous_ceo_tenure_was_missing  firm_rtn_1m_was_missing  \\\n0                                  False                    False   \n1                                  False                    False   \n2                                  False                    False   \n3                                  False                    False   \n4                                  False                    False   \n...                                  ...                      ...   \n1135510                             True                    False   \n1135511                             True                    False   \n1135512                             True                    False   \n1135513                             True                    False   \n1135514                             True                    False   \n\n         firm_rtn_3m_was_missing  firm_rtn_6m_was_missing  \\\n0                          False                    False   \n1                          False                    False   \n2                          False                    False   \n3                          False                    False   \n4                          False                    False   \n...                          ...                      ...   \n1135510                    False                     True   \n1135511                    False                     True   \n1135512                    False                     True   \n1135513                    False                    False   \n1135514                    False                    False   \n\n         firm_rtn_12m_was_missing  firm_rtn_24m_was_missing  \\\n0                           False                     False   \n1                           False                     False   \n2                           False                     False   \n3                           False                     False   \n4                           False                     False   \n...                           ...                       ...   \n1135510                      True                      True   \n1135511                      True                      True   \n1135512                      True                      True   \n1135513                      True                      True   \n1135514                      True                      True   \n\n         firm_rtn_36m_was_missing  sic_was_missing  sector_rtn_1m_was_missing  \\\n0                           False            False                      False   \n1                           False            False                      False   \n2                           False            False                      False   \n3                           False            False                      False   \n4                           False            False                      False   \n...                           ...              ...                        ...   \n1135510                      True            False                      False   \n1135511                      True            False                      False   \n1135512                      True            False                      False   \n1135513                      True            False                      False   \n1135514                      True            False                      False   \n\n         sector_rtn_3m_was_missing  sector_rtn_6m_was_missing  \\\n0                            False                      False   \n1                            False                      False   \n2                            False                      False   \n3                            False                      False   \n4                            False                      False   \n...                            ...                        ...   \n1135510                      False                      False   \n1135511                      False                      False   \n1135512                      False                      False   \n1135513                      False                      False   \n1135514                      False                      False   \n\n         sector_rtn_12m_was_missing  sector_rtn_24m_was_missing  \\\n0                             False                       False   \n1                             False                       False   \n2                             False                       False   \n3                             False                       False   \n4                             False                       False   \n...                             ...                         ...   \n1135510                       False                       False   \n1135511                       False                       False   \n1135512                       False                       False   \n1135513                       False                       False   \n1135514                       False                       False   \n\n         sector_rtn_36m_was_missing  start_date_year  start_date_month  \n0                             False             2018                 9  \n1                             False             2015                 5  \n2                             False             2018                 9  \n3                             False             2018                 9  \n4                             False             2018                 9  \n...                             ...              ...               ...  \n1135510                       False             2018                 9  \n1135511                       False             2018                 9  \n1135512                       False             2018                 9  \n1135513                       False             2018                 9  \n1135514                       False             2018                 9  \n\n[1135515 rows x 74 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>director_name</th>\n      <th>company_name</th>\n      <th>role_name</th>\n      <th>director_id</th>\n      <th>company_id</th>\n      <th>date_of_birth</th>\n      <th>gender</th>\n      <th>nationality</th>\n      <th>network_size</th>\n      <th>role_id</th>\n      <th>current_role</th>\n      <th>role_duration</th>\n      <th>past_roles_count</th>\n      <th>company_roles_count</th>\n      <th>past_roles_tenure_avg</th>\n      <th>company_roles_tenure_avg</th>\n      <th>ceo</th>\n      <th>chair</th>\n      <th>chair_ceo</th>\n      <th>past_role_duration</th>\n      <th>past_roles_tenure</th>\n      <th>company_roles_tenure</th>\n      <th>active_roles_count</th>\n      <th>active_roles_count_max</th>\n      <th>previous_ceo_tenure</th>\n      <th>role_extension</th>\n      <th>end_date</th>\n      <th>start_date</th>\n      <th>role_tenure</th>\n      <th>turnover</th>\n      <th>gvkey</th>\n      <th>tic</th>\n      <th>firm_rtn_1m</th>\n      <th>firm_rtn_3m</th>\n      <th>firm_rtn_6m</th>\n      <th>firm_rtn_12m</th>\n      <th>firm_rtn_24m</th>\n      <th>firm_rtn_36m</th>\n      <th>index_rtn_1m</th>\n      <th>index_rtn_3m</th>\n      <th>index_rtn_6m</th>\n      <th>index_rtn_12m</th>\n      <th>index_rtn_24m</th>\n      <th>index_rtn_36m</th>\n      <th>age</th>\n      <th>tenure_at_ceo_start</th>\n      <th>sic</th>\n      <th>sector_rtn_1m</th>\n      <th>sector_rtn_3m</th>\n      <th>sector_rtn_6m</th>\n      <th>sector_rtn_12m</th>\n      <th>sector_rtn_24m</th>\n      <th>sector_rtn_36m</th>\n      <th>nationality_was_missing</th>\n      <th>network_size_was_missing</th>\n      <th>past_roles_tenure_avg_was_missing</th>\n      <th>company_roles_tenure_avg_was_missing</th>\n      <th>previous_ceo_tenure_was_missing</th>\n      <th>firm_rtn_1m_was_missing</th>\n      <th>firm_rtn_3m_was_missing</th>\n      <th>firm_rtn_6m_was_missing</th>\n      <th>firm_rtn_12m_was_missing</th>\n      <th>firm_rtn_24m_was_missing</th>\n      <th>firm_rtn_36m_was_missing</th>\n      <th>sic_was_missing</th>\n      <th>sector_rtn_1m_was_missing</th>\n      <th>sector_rtn_3m_was_missing</th>\n      <th>sector_rtn_6m_was_missing</th>\n      <th>sector_rtn_12m_was_missing</th>\n      <th>sector_rtn_24m_was_missing</th>\n      <th>sector_rtn_36m_was_missing</th>\n      <th>start_date_year</th>\n      <th>start_date_month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018-09-30</td>\n      <td>A Beharelle</td>\n      <td>TRUEBLUE INC (Labor Ready Inc prior to 12/2007)</td>\n      <td>President/CEO</td>\n      <td>1340849</td>\n      <td>18329</td>\n      <td>1969-01-01</td>\n      <td>M</td>\n      <td>NaN</td>\n      <td>2669.0</td>\n      <td>218</td>\n      <td>True</td>\n      <td>608 days 00:00:00.000000000</td>\n      <td>7</td>\n      <td>2</td>\n      <td>1381 days 20:34:17.142857</td>\n      <td>762 days 00:00:00</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0 days</td>\n      <td>9673 days</td>\n      <td>1524 days</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1206 days</td>\n      <td>False</td>\n      <td>2020-05-31</td>\n      <td>2018-09-30</td>\n      <td>0 days</td>\n      <td>False</td>\n      <td>22207.0</td>\n      <td>TBI</td>\n      <td>-0.110922</td>\n      <td>-0.033395</td>\n      <td>0.005792</td>\n      <td>0.160356</td>\n      <td>0.149603</td>\n      <td>0.159324</td>\n      <td>0.000250</td>\n      <td>0.066419</td>\n      <td>0.102813</td>\n      <td>0.154530</td>\n      <td>0.344012</td>\n      <td>0.513402</td>\n      <td>49.745032</td>\n      <td>1524 days</td>\n      <td>7363</td>\n      <td>-0.063492</td>\n      <td>0.050542</td>\n      <td>0.088124</td>\n      <td>0.245874</td>\n      <td>0.334664</td>\n      <td>0.460395</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>2018</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2018-09-30</td>\n      <td>Steve Cooper</td>\n      <td>TRUEBLUE INC (Labor Ready Inc prior to 12/2007)</td>\n      <td>CEO</td>\n      <td>274662</td>\n      <td>18329</td>\n      <td>1962-01-01</td>\n      <td>M</td>\n      <td>American</td>\n      <td>721.0</td>\n      <td>5240630</td>\n      <td>False</td>\n      <td>1206 days 00:00:00.000000000</td>\n      <td>10</td>\n      <td>7</td>\n      <td>1229 days 14:24:00</td>\n      <td>1013 days 06:51:25.714285</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>1206 days</td>\n      <td>12296 days</td>\n      <td>7093 days</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3284 days</td>\n      <td>False</td>\n      <td>2018-09-30</td>\n      <td>2015-05-31</td>\n      <td>1218 days</td>\n      <td>True</td>\n      <td>22207.0</td>\n      <td>TBI</td>\n      <td>-0.110922</td>\n      <td>-0.033395</td>\n      <td>0.005792</td>\n      <td>0.160356</td>\n      <td>0.149603</td>\n      <td>0.159324</td>\n      <td>0.000250</td>\n      <td>0.066419</td>\n      <td>0.102813</td>\n      <td>0.154530</td>\n      <td>0.344012</td>\n      <td>0.513402</td>\n      <td>56.745861</td>\n      <td>2603 days</td>\n      <td>7363</td>\n      <td>-0.063492</td>\n      <td>0.050542</td>\n      <td>0.088124</td>\n      <td>0.245874</td>\n      <td>0.334664</td>\n      <td>0.460395</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>2015</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2018-10-31</td>\n      <td>A Beharelle</td>\n      <td>TRUEBLUE INC (Labor Ready Inc prior to 12/2007)</td>\n      <td>President/CEO</td>\n      <td>1340849</td>\n      <td>18329</td>\n      <td>1969-01-01</td>\n      <td>M</td>\n      <td>NaN</td>\n      <td>2669.0</td>\n      <td>218</td>\n      <td>True</td>\n      <td>608 days 00:00:00.000000000</td>\n      <td>7</td>\n      <td>2</td>\n      <td>1381 days 20:34:17.142857</td>\n      <td>762 days 00:00:00</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0 days</td>\n      <td>9673 days</td>\n      <td>1524 days</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1206 days</td>\n      <td>False</td>\n      <td>2020-05-31</td>\n      <td>2018-09-30</td>\n      <td>31 days</td>\n      <td>False</td>\n      <td>22207.0</td>\n      <td>TBI</td>\n      <td>-0.104415</td>\n      <td>-0.137523</td>\n      <td>-0.124578</td>\n      <td>-0.139114</td>\n      <td>0.333143</td>\n      <td>-0.194684</td>\n      <td>-0.074608</td>\n      <td>-0.043837</td>\n      <td>0.017711</td>\n      <td>0.046658</td>\n      <td>0.272652</td>\n      <td>0.299454</td>\n      <td>49.829908</td>\n      <td>1524 days</td>\n      <td>7363</td>\n      <td>-0.084437</td>\n      <td>-0.139710</td>\n      <td>0.017290</td>\n      <td>0.064980</td>\n      <td>0.300540</td>\n      <td>0.284561</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>2018</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2018-11-30</td>\n      <td>A Beharelle</td>\n      <td>TRUEBLUE INC (Labor Ready Inc prior to 12/2007)</td>\n      <td>President/CEO</td>\n      <td>1340849</td>\n      <td>18329</td>\n      <td>1969-01-01</td>\n      <td>M</td>\n      <td>NaN</td>\n      <td>2669.0</td>\n      <td>218</td>\n      <td>True</td>\n      <td>608 days 00:00:00.000000000</td>\n      <td>7</td>\n      <td>2</td>\n      <td>1381 days 20:34:17.142857</td>\n      <td>762 days 00:00:00</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0 days</td>\n      <td>9673 days</td>\n      <td>1524 days</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1206 days</td>\n      <td>False</td>\n      <td>2020-05-31</td>\n      <td>2018-09-30</td>\n      <td>61 days</td>\n      <td>False</td>\n      <td>22207.0</td>\n      <td>TBI</td>\n      <td>0.082297</td>\n      <td>-0.138225</td>\n      <td>-0.021318</td>\n      <td>-0.112478</td>\n      <td>0.205251</td>\n      <td>-0.137931</td>\n      <td>0.017702</td>\n      <td>-0.057991</td>\n      <td>0.009514</td>\n      <td>0.036178</td>\n      <td>0.242836</td>\n      <td>0.318132</td>\n      <td>49.912045</td>\n      <td>1524 days</td>\n      <td>7363</td>\n      <td>-0.023455</td>\n      <td>-0.167763</td>\n      <td>-0.058713</td>\n      <td>-0.043130</td>\n      <td>0.155166</td>\n      <td>0.253644</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>2018</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2018-12-31</td>\n      <td>A Beharelle</td>\n      <td>TRUEBLUE INC (Labor Ready Inc prior to 12/2007)</td>\n      <td>President/CEO</td>\n      <td>1340849</td>\n      <td>18329</td>\n      <td>1969-01-01</td>\n      <td>M</td>\n      <td>NaN</td>\n      <td>2669.0</td>\n      <td>218</td>\n      <td>True</td>\n      <td>608 days 00:00:00.000000000</td>\n      <td>7</td>\n      <td>2</td>\n      <td>1381 days 20:34:17.142857</td>\n      <td>762 days 00:00:00</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0 days</td>\n      <td>9673 days</td>\n      <td>1524 days</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1206 days</td>\n      <td>False</td>\n      <td>2020-05-31</td>\n      <td>2018-09-30</td>\n      <td>92 days</td>\n      <td>False</td>\n      <td>22207.0</td>\n      <td>TBI</td>\n      <td>-0.118812</td>\n      <td>-0.145873</td>\n      <td>-0.174397</td>\n      <td>-0.190909</td>\n      <td>-0.097363</td>\n      <td>-0.136258</td>\n      <td>-0.094623</td>\n      <td>-0.147341</td>\n      <td>-0.090708</td>\n      <td>-0.069899</td>\n      <td>0.105442</td>\n      <td>0.220573</td>\n      <td>49.996920</td>\n      <td>1524 days</td>\n      <td>7363</td>\n      <td>-0.129275</td>\n      <td>-0.226301</td>\n      <td>-0.194127</td>\n      <td>-0.135414</td>\n      <td>-0.031325</td>\n      <td>0.091186</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>2018</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1135510</th>\n      <td>2018-12-31</td>\n      <td>Zander Lurie</td>\n      <td>SVMK INC (SurveyMonkey)</td>\n      <td>CEO</td>\n      <td>549856</td>\n      <td>2920393</td>\n      <td>1974-01-01</td>\n      <td>M</td>\n      <td>NaN</td>\n      <td>1453.0</td>\n      <td>5833307</td>\n      <td>False</td>\n      <td>187 days 00:00:00.000000000</td>\n      <td>19</td>\n      <td>0</td>\n      <td>754 days 07:34:44.210526</td>\n      <td>NaT</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0 days</td>\n      <td>14332 days</td>\n      <td>0 days</td>\n      <td>3</td>\n      <td>5</td>\n      <td>NaT</td>\n      <td>False</td>\n      <td>2019-04-30</td>\n      <td>2018-09-30</td>\n      <td>92 days</td>\n      <td>True</td>\n      <td>34140.0</td>\n      <td>SVMK</td>\n      <td>-0.140756</td>\n      <td>-0.234560</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.094623</td>\n      <td>-0.147341</td>\n      <td>-0.090708</td>\n      <td>-0.069899</td>\n      <td>0.105442</td>\n      <td>0.220573</td>\n      <td>44.997502</td>\n      <td>0 days</td>\n      <td>7370</td>\n      <td>-0.063501</td>\n      <td>-0.160718</td>\n      <td>-0.125409</td>\n      <td>0.154433</td>\n      <td>0.520593</td>\n      <td>0.690130</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>2018</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1135511</th>\n      <td>2019-01-31</td>\n      <td>Zander Lurie</td>\n      <td>SVMK INC (SurveyMonkey)</td>\n      <td>CEO</td>\n      <td>549856</td>\n      <td>2920393</td>\n      <td>1974-01-01</td>\n      <td>M</td>\n      <td>NaN</td>\n      <td>1453.0</td>\n      <td>5833307</td>\n      <td>False</td>\n      <td>187 days 00:00:00.000000000</td>\n      <td>19</td>\n      <td>0</td>\n      <td>754 days 07:34:44.210526</td>\n      <td>NaT</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0 days</td>\n      <td>14332 days</td>\n      <td>0 days</td>\n      <td>3</td>\n      <td>5</td>\n      <td>NaT</td>\n      <td>False</td>\n      <td>2019-04-30</td>\n      <td>2018-09-30</td>\n      <td>123 days</td>\n      <td>True</td>\n      <td>34140.0</td>\n      <td>SVMK</td>\n      <td>0.060310</td>\n      <td>0.214753</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.084507</td>\n      <td>-0.000732</td>\n      <td>-0.044537</td>\n      <td>-0.040934</td>\n      <td>0.177848</td>\n      <td>0.404371</td>\n      <td>45.082377</td>\n      <td>0 days</td>\n      <td>7370</td>\n      <td>0.163315</td>\n      <td>0.098375</td>\n      <td>0.028457</td>\n      <td>0.248292</td>\n      <td>0.647609</td>\n      <td>1.269113</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>2018</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1135512</th>\n      <td>2019-02-28</td>\n      <td>Zander Lurie</td>\n      <td>SVMK INC (SurveyMonkey)</td>\n      <td>CEO</td>\n      <td>549856</td>\n      <td>2920393</td>\n      <td>1974-01-01</td>\n      <td>M</td>\n      <td>NaN</td>\n      <td>1453.0</td>\n      <td>5833307</td>\n      <td>False</td>\n      <td>187 days 00:00:00.000000000</td>\n      <td>19</td>\n      <td>0</td>\n      <td>754 days 07:34:44.210526</td>\n      <td>NaT</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0 days</td>\n      <td>14332 days</td>\n      <td>0 days</td>\n      <td>3</td>\n      <td>5</td>\n      <td>NaT</td>\n      <td>False</td>\n      <td>2019-04-30</td>\n      <td>2018-09-30</td>\n      <td>151 days</td>\n      <td>True</td>\n      <td>34140.0</td>\n      <td>SVMK</td>\n      <td>0.166026</td>\n      <td>0.062325</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.032998</td>\n      <td>0.014288</td>\n      <td>-0.044532</td>\n      <td>0.030671</td>\n      <td>0.175642</td>\n      <td>0.454904</td>\n      <td>45.159038</td>\n      <td>0 days</td>\n      <td>7370</td>\n      <td>0.085447</td>\n      <td>0.180294</td>\n      <td>0.008292</td>\n      <td>0.267708</td>\n      <td>0.791593</td>\n      <td>1.471535</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>2018</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1135513</th>\n      <td>2019-03-31</td>\n      <td>Zander Lurie</td>\n      <td>SVMK INC (SurveyMonkey)</td>\n      <td>CEO</td>\n      <td>549856</td>\n      <td>2920393</td>\n      <td>1974-01-01</td>\n      <td>M</td>\n      <td>NaN</td>\n      <td>1453.0</td>\n      <td>5833307</td>\n      <td>False</td>\n      <td>187 days 00:00:00.000000000</td>\n      <td>19</td>\n      <td>0</td>\n      <td>754 days 07:34:44.210526</td>\n      <td>NaT</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0 days</td>\n      <td>14332 days</td>\n      <td>0 days</td>\n      <td>3</td>\n      <td>5</td>\n      <td>NaT</td>\n      <td>False</td>\n      <td>2019-04-30</td>\n      <td>2018-09-30</td>\n      <td>182 days</td>\n      <td>True</td>\n      <td>34140.0</td>\n      <td>SVMK</td>\n      <td>0.200396</td>\n      <td>0.484108</td>\n      <td>0.135995</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.013028</td>\n      <td>0.134888</td>\n      <td>-0.032327</td>\n      <td>0.067163</td>\n      <td>0.192087</td>\n      <td>0.379414</td>\n      <td>45.243913</td>\n      <td>0 days</td>\n      <td>7370</td>\n      <td>0.006231</td>\n      <td>0.270859</td>\n      <td>0.038207</td>\n      <td>0.250032</td>\n      <td>0.794134</td>\n      <td>1.228860</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>2018</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1135514</th>\n      <td>2019-04-30</td>\n      <td>Zander Lurie</td>\n      <td>SVMK INC (SurveyMonkey)</td>\n      <td>CEO</td>\n      <td>549856</td>\n      <td>2920393</td>\n      <td>1974-01-01</td>\n      <td>M</td>\n      <td>NaN</td>\n      <td>1453.0</td>\n      <td>5833307</td>\n      <td>False</td>\n      <td>187 days 00:00:00.000000000</td>\n      <td>20</td>\n      <td>1</td>\n      <td>725 days 22:48:00</td>\n      <td>187 days 00:00:00</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>187 days</td>\n      <td>14519 days</td>\n      <td>187 days</td>\n      <td>3</td>\n      <td>5</td>\n      <td>NaT</td>\n      <td>True</td>\n      <td>2019-04-30</td>\n      <td>2018-09-30</td>\n      <td>212 days</td>\n      <td>False</td>\n      <td>34140.0</td>\n      <td>SVMK</td>\n      <td>-0.017024</td>\n      <td>0.375865</td>\n      <td>0.671335</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.038843</td>\n      <td>0.087103</td>\n      <td>0.086307</td>\n      <td>0.105547</td>\n      <td>0.226712</td>\n      <td>0.425706</td>\n      <td>45.326051</td>\n      <td>0 days</td>\n      <td>7370</td>\n      <td>0.038043</td>\n      <td>0.139584</td>\n      <td>0.228622</td>\n      <td>0.275216</td>\n      <td>0.741504</td>\n      <td>1.237217</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>2018</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n<p>1135515 rows × 74 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "# Convert DOB and start date to year and month\n",
    "df_ceos = df_ceos.assign(dob_year =df_ceos.date_of_birth.dt.year,\n",
    "               dob_month =df_ceos.date_of_birth.dt.month)\n",
    "\n",
    "df_ceos = df_ceos.assign(start_date_year =df_ceos.start_date.dt.year,\n",
    "               start_date_month =df_ceos.start_date.dt.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing target, separate target from predictors\n",
    "y = df_ceos['turnover']\n",
    "df_ceos.drop(drop_cols, axis=1, inplace=True)\n",
    "df_ceos.drop(['turnover'], axis=1, inplace=True)\n",
    "\n",
    "# \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "categorical_cols = [cname for cname in X_train_full.columns if\n",
    "                    X_train_full[cname].nunique() < 10 and \n",
    "                    X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if \n",
    "                X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = categorical_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "X_test = X_test_full[my_cols].copy()\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('model', model)\n",
    "                     ])\n",
    "# ==========\n",
    "\n",
    "# Make copy to avoid changing original data (when imputing)\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_valid.copy()\n",
    "\n",
    "# Make new columns indicating what will be imputed\n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "print(\"MAE from Approach 3 (One-Hot Encoding):\") \n",
    "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Use robust scalar to substract mean and divide by a variance measure for numerical cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "feature_cols = baseline_data.columns.drop('outcome')\n",
    "\n",
    "# Keep 5 features\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "\n",
    "X_new = selector.fit_transform(baseline_data[feature_cols], baseline_data['outcome'])\n",
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Frequency tables for each categorical feature\n",
    "for column in churn.select_dtypes(include=['object']).columns:\n",
    "    display(pd.crosstab(index=churn[column], columns='% observations', normalize='columns'))\n",
    "\n",
    "# Histograms for each numeric features\n",
    "display(churn.describe())\n",
    "%matplotlib inline\n",
    "hist = churn.hist(bins=30, sharey=True, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see immediately that:\n",
    "- `State` appears to be quite evenly distributed\n",
    "- `Phone` takes on too many unique values to be of any practical use.  It's possible parsing out the prefix could have some value, but without more context on how these are allocated, we should avoid using it.\n",
    "- Only 14% of customers churned, so there is some class imabalance, but nothing extreme.\n",
    "- Most of the numeric features are surprisingly nicely distributed, with many showing bell-like gaussianity.  `VMail Message` being a notable exception (and `Area Code` showing up as a feature we should convert to non-numeric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "churn = churn.drop('Phone', axis=1)\n",
    "churn['Area Code'] = churn['Area Code'].astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's look at the relationship between each of the features and our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for column in churn.select_dtypes(include=['object']).columns:\n",
    "    if column != 'Churn?':\n",
    "        display(pd.crosstab(index=churn[column], columns=churn['Churn?'], normalize='columns'))\n",
    "\n",
    "for column in churn.select_dtypes(exclude=['object']).columns:\n",
    "    print(column)\n",
    "    hist = churn[[column, 'Churn?']].hist(by='Churn?', bins=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly we see that churners appear:\n",
    "- Fairly evenly distributed geographically\n",
    "- More likely to have an international plan\n",
    "- Less likely to have a voicemail plan\n",
    "- To exhibit some bimodality in daily minutes (either higher or lower than the average for non-churners)\n",
    "- To have a larger number of customer service calls (which makes sense as we'd expect customers who experience lots of problems may be more likely to churn)\n",
    "\n",
    "In addition, we see that churners take on very similar distributions for features like `Day Mins` and `Day Charge`.  That's not surprising as we'd expect minutes spent talking to correlate with charges.  Let's dig deeper into the relationships between our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(churn.corr())\n",
    "pd.plotting.scatter_matrix(churn, figsize=(12, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see several features that essentially have 100% correlation with one another.  Including these feature pairs in some machine learning algorithms can create catastrophic problems, while in others it will only introduce minor redundancy and bias.  Let's remove one feature from each of the highly correlated pairs: Day Charge from the pair with Day Mins, Night Charge from the pair with Night Mins, Intl Charge from the pair with Intl Mins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "churn = churn.drop(['Day Charge', 'Eve Charge', 'Night Charge', 'Intl Charge'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've cleaned up our dataset, let's determine which algorithm to use.  As mentioned above, there appear to be some variables where both high and low (but not intermediate) values are predictive of churn.  In order to accommodate this in an algorithm like linear regression, we'd need to generate polynomial (or bucketed) terms.  Instead, let's attempt to model this problem using gradient boosted trees.  Amazon SageMaker provides an XGBoost container that we can use to train in a managed, distributed setting, and then host as a real-time prediction endpoint.  XGBoost uses gradient boosted trees which naturally account for non-linear relationships between features and the target variable, as well as accommodating complex interactions between features.\n",
    "\n",
    "Amazon SageMaker XGBoost can train on data in either a CSV or LibSVM format.  For this example, we'll stick with CSV.  It should:\n",
    "- Have the predictor variable in the first column\n",
    "- Not have a header row\n",
    "\n",
    "But first, let's convert our categorical features into numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_data = pd.get_dummies(churn)\n",
    "model_data = pd.concat([model_data['Churn?_True.'], model_data.drop(['Churn?_False.', 'Churn?_True.'], axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's split the data into training, validation, and test sets.  This will help prevent us from overfitting the model, and allow us to test the models accuracy on data it hasn't already seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data)), int(0.9 * len(model_data))])\n",
    "train_data.to_csv('train.csv', header=False, index=False)\n",
    "validation_data.to_csv('validation.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll upload these files to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train\n",
    "\n",
    "Moving onto training, first we'll need to specify the locations of the XGBoost algorithm containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, because we're training with the CSV file format, we'll create `s3_input`s that our training function can use as a pointer to the files in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can specify a few parameters like what type of training instances we'd like to use and how many, as well as our XGBoost hyperparameters.  A few key hyperparameters are:\n",
    "- `max_depth` controls how deep each tree within the algorithm can be built.  Deeper trees can lead to better fit, but are more computationally expensive and can lead to overfitting.  There is typically some trade-off in model performance that needs to be explored between a large number of shallow trees and a smaller number of deeper trees.\n",
    "- `subsample` controls sampling of the training data.  This technique can help reduce overfitting, but setting it too low can also starve the model of data.\n",
    "- `num_round` controls the number of boosting rounds.  This is essentially the subsequent models that are trained using the residuals of previous iterations.  Again, more rounds should produce a better fit on the training data, but can be computationally expensive or lead to overfitting.\n",
    "- `eta` controls how aggressive each round of boosting is.  Larger values lead to more conservative boosting.\n",
    "- `gamma` controls how aggressively trees are grown.  Larger values lead to more conservative models.\n",
    "\n",
    "More detail on XGBoost's hyperparmeters can be found on their GitHub [page](https://github.com/dmlc/xgboost/blob/master/doc/parameter.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Compile\n",
    "[Amazon SageMaker Neo](https://aws.amazon.com/sagemaker/neo/) optimizes models to run up to twice as fast, with no loss in accuracy. When calling `compile_model()` function, we specify the target instance family (m4) as well as the S3 bucket to which the compiled model would be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = xgb\n",
    "try:\n",
    "    xgb.create_model()._neo_image_account(boto3.Session().region_name)\n",
    "except:\n",
    "    print('Neo is not currently supported in', boto3.Session().region_name)\n",
    "else:\n",
    "    output_path = '/'.join(xgb.output_path.split('/')[:-1])\n",
    "    compiled_model = xgb.compile_model(target_instance_family='ml_m4', \n",
    "                                   input_shape={'data':[1, 69]},\n",
    "                                   role=role,\n",
    "                                   framework='xgboost',\n",
    "                                   framework_version='0.7',\n",
    "                                   output_path=output_path)\n",
    "    compiled_model.name = 'deployed-xgboost-customer-churn'\n",
    "    compiled_model.image = get_image_uri(sess.boto_region_name, 'xgboost-neo', repo_version='latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Host\n",
    "\n",
    "Now that we've trained the algorithm, let's create a model and deploy it to a hosted endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_predictor = compiled_model.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Now that we have a hosted endpoint running, we can make real-time predictions from our model very easily, simply by making an http POST request.  But first, we'll need to setup serializers and deserializers for passing our `test_data` NumPy arrays to the model behind the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use a simple function to:\n",
    "1. Loop over our test dataset\n",
    "1. Split it into mini-batches of rows \n",
    "1. Convert those mini-batchs to CSV string payloads\n",
    "1. Retrieve mini-batch predictions by invoking the XGBoost endpoint\n",
    "1. Collect predictions and convert from the CSV output our model provides into a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "predictions = predict(test_data.as_matrix()[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to compare the performance of a machine learning model, but let's start by simply by comparing actual to predicted values.  In this case, we're simply predicting whether the customer churned (`1`) or not (`0`), which produces a simple confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(index=test_data.iloc[:, 0], columns=np.round(predictions), rownames=['actual'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note, due to randomized elements of the algorithm, you results may differ slightly._\n",
    "\n",
    "Of the 48 churners, we've correctly predicted 39 of them (true positives). And, we incorrectly predicted 4 customers would churn who then ended up not doing so (false positives).  There are also 9 customers who ended up churning, that we predicted would not (false negatives).\n",
    "\n",
    "An important point here is that because of the `np.round()` function above we are using a simple threshold (or cutoff) of 0.5.  Our predictions from `xgboost` come out as continuous values between 0 and 1 and we force them into the binary classes that we began with.  However, because a customer that churns is expected to cost the company more than proactively trying to retain a customer who we think might churn, we should consider adjusting this cutoff.  That will almost certainly increase the number of false positives, but it can also be expected to increase the number of true positives and reduce the number of false negatives.\n",
    "\n",
    "To get a rough intuition here, let's look at the continuous values of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The continuous valued predictions coming from our model tend to skew toward 0 or 1, but there is sufficient mass between 0.1 and 0.9 that adjusting the cutoff should indeed shift a number of customers' predictions.  For example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(index=test_data.iloc[:, 0], columns=np.where(predictions > 0.3, 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that changing the cutoff from 0.5 to 0.3 results in 1 more true positives, 3 more false positives, and 1 fewer false negatives.  The numbers are small overall here, but that's 6-10% of customers overall that are shifting because of a change to the cutoff.  Was this the right decision?  We may end up retaining 3 extra customers, but we also unnecessarily incentivized 5 more customers who would have stayed.  Determining optimal cutoffs is a key step in properly applying machine learning in a real-world setting.  Let's discuss this more broadly and then apply a specific, hypothetical solution for our current problem.\n",
    "\n",
    "### Relative cost of errors\n",
    "\n",
    "Any practical binary classification problem is likely to produce a similarly sensitive cutoff. That by itself isn’t a problem. After all, if the scores for two classes are really easy to separate, the problem probably isn’t very hard to begin with and might even be solvable with simple rules instead of ML.\n",
    "\n",
    "More important, if I put an ML model into production, there are costs associated with the model erroneously assigning false positives and false negatives. I also need to look at similar costs associated with correct predictions of true positives and true negatives.  Because the choice of the cutoff affects all four of these statistics, I need to consider the relative costs to the business for each of these four outcomes for each prediction.\n",
    "\n",
    "#### Assigning costs\n",
    "\n",
    "What are the costs for our problem of mobile operator churn? The costs, of course, depend on the specific actions that the business takes. Let's make some assumptions here.\n",
    "\n",
    "First, assign the true negatives the cost of \\$0. Our model essentially correctly identified a happy customer in this case, and we don’t need to do anything.\n",
    "\n",
    "False negatives are the most problematic, because they incorrectly predict that a churning customer will stay. We lose the customer and will have to pay all the costs of acquiring a replacement customer, including foregone revenue, advertising costs, administrative costs, point of sale costs, and likely a phone hardware subsidy. A quick search on the Internet reveals that such costs typically run in the hundreds of dollars so, for the purposes of this example, let's assume \\$500. This is the cost of false negatives.\n",
    "\n",
    "Finally, for customers that our model identifies as churning, let's assume a retention incentive in the amount of \\$100. If my provider offered me such a concession, I’d certainly think twice before leaving. This is the cost of both true positive and false positive outcomes. In the case of false positives (the customer is happy, but the model mistakenly predicted churn), we will “waste” the \\$100 concession. We probably could have spent that \\$100 more effectively, but it's possible we increased the loyalty of an already loyal customer, so that’s not so bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the optimal cutoff\n",
    "\n",
    "It’s clear that false negatives are substantially more costly than false positives. Instead of optimizing for error based on the number of customers, we should be minimizing a cost function that looks like this:\n",
    "\n",
    "```txt\n",
    "$500 * FN(C) + $0 * TN(C) + $100 * FP(C) + $100 * TP(C)\n",
    "```\n",
    "\n",
    "FN(C) means that the false negative percentage is a function of the cutoff, C, and similar for TN, FP, and TP.  We need to find the cutoff, C, where the result of the expression is smallest.\n",
    "\n",
    "A straightforward way to do this, is to simply run a simulation over a large number of possible cutoffs.  We test 100 possible values in the for loop below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cutoffs = np.arange(0.01, 1, 0.01)\n",
    "costs = []\n",
    "for c in cutoffs:\n",
    "    costs.append(np.sum(np.sum(np.array([[0, 100], [500, 100]]) * \n",
    "                               pd.crosstab(index=test_data.iloc[:, 0], \n",
    "                                           columns=np.where(predictions > c, 1, 0)))))\n",
    "\n",
    "costs = np.array(costs)\n",
    "plt.plot(cutoffs, costs)\n",
    "plt.show()\n",
    "print('Cost is minimized near a cutoff of:', cutoffs[np.argmin(costs)], 'for a cost of:', np.min(costs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above chart shows how picking a threshold too low results in costs skyrocketing as all customers are given a retention incentive.  Meanwhile, setting the threshold too high results in too many lost customers, which ultimately grows to be nearly as costly.  The overall cost can be minimized at \\$8400 by setting the cutoff to 0.46, which is substantially better than the \\$20k+ I would expect to lose by not taking any action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Extensions\n",
    "\n",
    "This notebook showcased how to build a model that predicts whether a customer is likely to churn, and then how to optimally set a threshold that accounts for the cost of true positives, false positives, and false negatives.  There are several means of extending it including:\n",
    "- Some customers who receive retention incentives will still churn.  Including a probability of churning despite receiving an incentive in our cost function would provide a better ROI on our retention programs.\n",
    "- Customers who switch to a lower-priced plan or who deactivate a paid feature represent different kinds of churn that could be modeled separately.\n",
    "- Modeling the evolution of customer behavior. If usage is dropping and the number of calls placed to Customer Service is increasing, you are more likely to experience churn then if the trend is the opposite. A customer profile should incorporate behavior trends.\n",
    "- Actual training data and monetary cost assignments could be more complex.\n",
    "- Multiple models for each type of churn could be needed.\n",
    "\n",
    "Regardless of additional complexity, similar principles described in this notebook are likely apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Clean-up\n",
    "\n",
    "If you're ready to be done with this notebook, please run the cell below.  This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('ml-turnover': pipenv)",
   "language": "python",
   "name": "python_defaultSpec_1594300779214"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}